{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "file1_path = '/content/cdata.csv'\n",
        "file2_path = '/content/cdata2.csv'\n",
        "\n",
        "data1 = pd.read_csv(file1_path)\n",
        "data2 = pd.read_csv(file2_path)\n",
        "\n",
        "\n",
        "combined_data = pd.concat([data1, data2], ignore_index=True)"
      ],
      "metadata": {
        "id": "xdd3FXZ6N4uI"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_text_after_question_mark(text):\n",
        "    return text.split('?')[0] + '?' if '?' in text else text\n",
        "\n",
        "combined_data['user_query'] = combined_data['user_query'].apply(remove_text_after_question_mark)\n",
        "\n",
        "combined_data['bot_response'].fillna('missing_response', inplace=True)\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-2.7B')\n",
        "\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def tokenize_data(data):\n",
        "    return tokenizer(data, return_tensors='pt', padding=True, truncation=True, max_length=tokenizer.model_max_length)\n",
        "\n",
        "tokenized_queries = tokenize_data(combined_data['user_query'].tolist())\n",
        "tokenized_responses = tokenize_data(combined_data['bot_response'].tolist())\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(combined_data, test_size=0.1)\n",
        "tokenized_train_queries = tokenize_data(train_data['user_query'].tolist())\n",
        "tokenized_train_responses = tokenize_data(train_data['bot_response'].tolist())\n",
        "tokenized_val_queries = tokenize_data(val_data['user_query'].tolist())\n",
        "tokenized_val_responses = tokenize_data(val_data['bot_response'].tolist())"
      ],
      "metadata": {
        "id": "NKfD4vBlN864"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class QueryDataset(Dataset):\n",
        "    def __init__(self, queries, responses):\n",
        "        self.queries = queries\n",
        "        self.responses = responses\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.queries['input_ids'][idx],\n",
        "            'attention_mask': self.queries['attention_mask'][idx],\n",
        "            'labels': self.responses['input_ids'][idx]\n",
        "        }\n",
        "\n",
        "\n",
        "train_dataset = QueryDataset(tokenized_train_queries, tokenized_train_responses)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "val_dataset = QueryDataset(tokenized_val_queries, tokenized_val_responses)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "HL5n-vu5OFBB"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "issWfdnHbLUU",
        "outputId": "e6999561-c78f-4fed-baca-e22b0b438348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      device = idx.device\n",
        "      b, t = idx.size()\n",
        "      assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "      pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "      tok_emb = self.transformer.wte(idx)\n",
        "      pos_emb = self.transformer.wpe(pos)\n",
        "      x = self.transformer.drop(tok_emb + pos_emb)\n",
        "      for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "      x = self.transformer.ln_f(x)\n",
        "\n",
        "      if targets is not None:\n",
        "        logits = self.lm_head(x)\n",
        "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
        "        targets_flat = targets.reshape(-1)\n",
        "        loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=-1)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        mask = (targets != -1)\n",
        "        num_correct = (preds == targets) & mask\n",
        "        accuracy = num_correct.sum().item() / mask.sum().item()\n",
        "      else:\n",
        "\n",
        "        logits = self.lm_head(x[:, [-1], :])\n",
        "        loss = None\n",
        "        accuracy = None\n",
        "\n",
        "      return logits, loss, accuracy\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.2, top_k=50, tokenizer=None):\n",
        "        generated_tokens = []\n",
        "        min_length = 20\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                values, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < values[:, [-1]]] = float('-inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            if len(generated_tokens) > min_length and next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "            generated_tokens.append(next_token.item())\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx, generated_tokens\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def beam_search_generate(self, input_ids, beam_size=3, max_length=50, length_penalty=1.0):\n",
        "        batch_size = input_ids.size(0)\n",
        "        input_length = input_ids.size(1)\n",
        "\n",
        "\n",
        "        generated = input_ids.unsqueeze(1).expand(batch_size, beam_size, input_length).contiguous().view(-1, input_length)\n",
        "        beam_scores = torch.zeros((batch_size, beam_size), dtype=torch.float, device=input_ids.device)\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view(-1)\n",
        "\n",
        "        for _ in range(max_length - input_length):\n",
        "            next_token_logits, _, _ = self(generated)\n",
        "            next_token_logits = next_token_logits[:, -1, :]\n",
        "\n",
        "            scores = next_token_logits + beam_scores[:, None].expand_as(next_token_logits)\n",
        "            next_token_scores = scores.view(batch_size, beam_size * next_token_logits.size(-1))\n",
        "            next_token_scores, next_tokens = torch.topk(next_token_scores, beam_size, dim=1, largest=True, sorted=True)\n",
        "\n",
        "            next_indices = next_tokens // next_token_logits.size(-1)\n",
        "            next_tokens = next_tokens % next_token_logits.size(-1)\n",
        "\n",
        "            beam_outputs = []\n",
        "            for batch_idx in range(batch_size):\n",
        "                for beam_idx in range(beam_size):\n",
        "                    beam_outputs.append(generated[batch_idx * beam_size + next_indices[batch_idx, beam_idx]])\n",
        "\n",
        "            generated = torch.stack(beam_outputs, dim=0).view(batch_size * beam_size, -1)\n",
        "            next_tokens = next_tokens.view(batch_size * beam_size, 1)\n",
        "            generated = torch.cat([generated, next_tokens], dim=-1)\n",
        "\n",
        "            beam_scores = next_token_scores.view(batch_size * beam_size)\n",
        "\n",
        "        generated = generated.view(batch_size, beam_size, -1)\n",
        "        best_beams = generated[:, 0, :]\n",
        "\n",
        "        return best_beams\n",
        "\n",
        "# Initialize the model\n",
        "config = GPTConfig()\n",
        "model = GPT(config)\n",
        "model.to('cuda')\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        labels = batch['labels'].to('cuda')\n",
        "\n",
        "\n",
        "        labels = labels[:, :input_ids.size(1)]\n",
        "\n",
        "        logits, loss, accuracy = model(input_ids, targets=labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    avg_accuracy = total_accuracy / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss:.4f}, Training Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            attention_mask = batch['attention_mask'].to('cuda')\n",
        "            labels = batch['labels'].to('cuda')\n",
        "\n",
        "            # Align the length of input_ids and labels\n",
        "            labels = labels[:, :input_ids.size(1)]\n",
        "\n",
        "            logits, loss, accuracy = model(input_ids, targets=labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_accuracy += accuracy\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    avg_val_accuracy = val_accuracy / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYPlCFxQONzZ",
        "outputId": "bd1c4cba-9a1c-41b5-d05b-4c269b919fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 107/107 [00:07<00:00, 14.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 3.8438, Training Accuracy: 0.2216\n",
            "Epoch 1, Validation Loss: 4.6934, Validation Accuracy: 0.2319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 107/107 [00:06<00:00, 15.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 3.5909, Training Accuracy: 0.2325\n",
            "Epoch 2, Validation Loss: 4.6219, Validation Accuracy: 0.2486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 107/107 [00:07<00:00, 14.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 3.3944, Training Accuracy: 0.2460\n",
            "Epoch 3, Validation Loss: 4.6605, Validation Accuracy: 0.2542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 107/107 [00:06<00:00, 15.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 3.2402, Training Accuracy: 0.2532\n",
            "Epoch 4, Validation Loss: 4.6212, Validation Accuracy: 0.2653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 107/107 [00:07<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 3.1060, Training Accuracy: 0.2615\n",
            "Epoch 5, Validation Loss: 4.6209, Validation Accuracy: 0.2472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 107/107 [00:07<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training Loss: 3.0092, Training Accuracy: 0.2604\n",
            "Epoch 6, Validation Loss: 4.6893, Validation Accuracy: 0.2750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 107/107 [00:07<00:00, 14.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training Loss: 2.9158, Training Accuracy: 0.2636\n",
            "Epoch 7, Validation Loss: 4.6900, Validation Accuracy: 0.2708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 107/107 [00:07<00:00, 14.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training Loss: 2.7894, Training Accuracy: 0.2679\n",
            "Epoch 8, Validation Loss: 4.6503, Validation Accuracy: 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 107/107 [00:06<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training Loss: 2.7313, Training Accuracy: 0.2732\n",
            "Epoch 9, Validation Loss: 4.7227, Validation Accuracy: 0.2444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 107/107 [00:07<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 2.6145, Training Accuracy: 0.2871\n",
            "Epoch 10, Validation Loss: 4.7589, Validation Accuracy: 0.2556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 107/107 [00:06<00:00, 15.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training Loss: 2.5717, Training Accuracy: 0.2816\n",
            "Epoch 11, Validation Loss: 4.8340, Validation Accuracy: 0.2514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 107/107 [00:07<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training Loss: 2.4910, Training Accuracy: 0.2923\n",
            "Epoch 12, Validation Loss: 4.7612, Validation Accuracy: 0.2889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 107/107 [00:06<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training Loss: 2.4199, Training Accuracy: 0.3031\n",
            "Epoch 13, Validation Loss: 4.9115, Validation Accuracy: 0.2486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 107/107 [00:07<00:00, 15.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training Loss: 2.4340, Training Accuracy: 0.3035\n",
            "Epoch 14, Validation Loss: 4.8450, Validation Accuracy: 0.2778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 107/107 [00:07<00:00, 15.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training Loss: 2.3971, Training Accuracy: 0.3107\n",
            "Epoch 15, Validation Loss: 4.8255, Validation Accuracy: 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 107/107 [00:06<00:00, 15.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training Loss: 2.3172, Training Accuracy: 0.3169\n",
            "Epoch 16, Validation Loss: 4.8954, Validation Accuracy: 0.2583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17:  54%|█████▍    | 58/107 [00:03<00:03, 14.34it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "file_path = '/content/combined_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "data.dropna(subset=['user_query', 'topic'], inplace=True)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['user_query'], data['topic'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "report_svm = classification_report(y_test, y_pred_svm, zero_division=1)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(report_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOzGVUkX1UQM",
        "outputId": "8009549e-7f2d-4a81-f77d-403599a06aa8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "        Awards - environment       1.00      1.00      1.00         2\n",
            "           Awards - medicine       1.00      1.00      1.00         3\n",
            "         Awards - technology       1.00      1.00      1.00         2\n",
            "         Culture - Christmas       1.00      1.00      1.00         8\n",
            "            Culture - Diwali       1.00      1.00      1.00         8\n",
            "               Culture - Eid       1.00      1.00      1.00         4\n",
            "          Culture - Hanukkah       1.00      1.00      1.00         5\n",
            "        Economy - technology       1.00      1.00      1.00         7\n",
            "          Education - Canada       1.00      0.00      0.00         1\n",
            "          Education - France       1.00      0.00      0.00         2\n",
            "         Education - Germany       1.00      0.00      0.00         1\n",
            "           Education - India       1.00      0.00      0.00         1\n",
            "           Education - Japan       1.00      0.00      0.00         1\n",
            "             Education - USA       0.00      1.00      0.00         0\n",
            "        Education - srilanka       1.00      0.00      0.00         1\n",
            "      Entertainment - action       1.00      0.00      0.00         1\n",
            "      Entertainment - comedy       1.00      0.00      0.00         1\n",
            "      Entertainment - horror       0.00      1.00      0.00         0\n",
            "Environment - climate change       1.00      1.00      1.00         3\n",
            "       Geography - Australia       1.00      0.00      0.00         1\n",
            "          Geography - Canada       1.00      0.00      0.00         1\n",
            "              Geography - UK       1.00      0.00      0.00         1\n",
            "             Geography - USA       0.00      1.00      0.00         0\n",
            "             Health - cancer       1.00      1.00      1.00         1\n",
            "       Health - hypertension       1.00      1.00      1.00         1\n",
            "         History - Australia       1.00      0.00      0.00         1\n",
            "            History - France       1.00      0.00      0.00         1\n",
            "             History - India       1.00      0.00      0.00         1\n",
            "                History - UK       1.00      0.00      0.00         1\n",
            "               History - USA       0.00      1.00      0.00         0\n",
            "          History - srilanka       1.00      0.00      0.00         1\n",
            "          Literature - India       1.00      0.00      0.00         1\n",
            "             Literature - UK       1.00      0.00      0.00         2\n",
            "            Literature - USA       0.00      1.00      0.00         0\n",
            "        Politics - Australia       0.00      1.00      0.00         0\n",
            "           Politics - Canada       1.00      0.00      0.00         3\n",
            "           Politics - France       1.00      0.00      0.00         3\n",
            "          Politics - Germany       1.00      1.00      1.00         1\n",
            "            Politics - India       1.00      0.00      0.00         3\n",
            "            Politics - Japan       1.00      1.00      1.00         1\n",
            "               Politics - UK       1.00      0.00      0.00         3\n",
            "              Politics - USA       1.00      0.50      0.67         2\n",
            "       Science - environment       1.00      0.00      0.00         1\n",
            "          Science - medicine       1.00      0.00      0.00         1\n",
            "        Science - technology       0.00      1.00      0.00         0\n",
            "         Sports - basketball       1.00      1.00      1.00         2\n",
            "            Sports - cricket       1.00      1.00      1.00         2\n",
            "           Sports - football       1.00      1.00      1.00         1\n",
            "              Sports - rugby       1.00      1.00      1.00         2\n",
            "    Technology - environment       0.00      1.00      0.00         0\n",
            "       Technology - medicine       1.00      0.00      0.00         1\n",
            "          Technology - space       1.00      0.00      0.00         1\n",
            "     Technology - technology       1.00      0.00      0.00         2\n",
            "          Travel - Australia       1.00      0.00      0.00         1\n",
            "            Travel - Germany       1.00      0.00      0.00         1\n",
            "                Travel - USA       0.00      1.00      0.00         0\n",
            "\n",
            "                    accuracy                           0.57        95\n",
            "                   macro avg       0.84      0.47      0.32        95\n",
            "                weighted avg       1.00      0.57      0.57        95\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "report_rf = classification_report(y_test, y_pred_rf, zero_division=1)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(report_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FWOTJE1_jEy",
        "outputId": "1bf9c907-5176-4f82-f4cc-4ad73a5ec910"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "        Awards - environment       1.00      1.00      1.00         2\n",
            "           Awards - medicine       1.00      1.00      1.00         3\n",
            "         Awards - technology       1.00      1.00      1.00         2\n",
            "         Culture - Christmas       1.00      1.00      1.00         8\n",
            "            Culture - Diwali       1.00      1.00      1.00         8\n",
            "               Culture - Eid       1.00      1.00      1.00         4\n",
            "          Culture - Hanukkah       1.00      1.00      1.00         5\n",
            "        Economy - technology       1.00      1.00      1.00         7\n",
            "       Education - Australia       0.00      1.00      0.00         0\n",
            "          Education - Canada       0.00      0.00      1.00         1\n",
            "          Education - France       1.00      0.00      0.00         2\n",
            "         Education - Germany       1.00      0.00      0.00         1\n",
            "           Education - India       1.00      0.00      0.00         1\n",
            "           Education - Japan       1.00      0.00      0.00         1\n",
            "              Education - UK       0.00      1.00      0.00         0\n",
            "        Education - srilanka       1.00      1.00      1.00         1\n",
            "      Entertainment - action       1.00      0.00      0.00         1\n",
            "      Entertainment - comedy       1.00      0.00      0.00         1\n",
            "      Entertainment - horror       0.00      1.00      0.00         0\n",
            "Environment - climate change       1.00      1.00      1.00         3\n",
            "       Geography - Australia       1.00      1.00      1.00         1\n",
            "          Geography - Canada       1.00      0.00      0.00         1\n",
            "          Geography - France       0.00      1.00      0.00         0\n",
            "         Geography - Germany       0.00      1.00      0.00         0\n",
            "              Geography - UK       1.00      0.00      0.00         1\n",
            "             Health - cancer       0.00      0.00      1.00         1\n",
            "       Health - hypertension       0.00      0.00      1.00         1\n",
            "         History - Australia       1.00      0.00      0.00         1\n",
            "            History - Canada       0.00      1.00      0.00         0\n",
            "            History - France       1.00      0.00      0.00         1\n",
            "           History - Germany       0.00      1.00      0.00         0\n",
            "             History - India       0.00      0.00      1.00         1\n",
            "                History - UK       1.00      0.00      0.00         1\n",
            "          History - srilanka       1.00      0.00      0.00         1\n",
            "      Literature - Australia       0.00      1.00      0.00         0\n",
            "         Literature - France       0.00      1.00      0.00         0\n",
            "          Literature - India       1.00      0.00      0.00         1\n",
            "             Literature - UK       1.00      0.00      0.00         2\n",
            "        Politics - Australia       0.00      1.00      0.00         0\n",
            "           Politics - Canada       0.00      0.00      1.00         3\n",
            "           Politics - France       0.00      0.00      1.00         3\n",
            "          Politics - Germany       0.50      1.00      0.67         1\n",
            "            Politics - India       1.00      0.00      0.00         3\n",
            "            Politics - Japan       0.50      1.00      0.67         1\n",
            "               Politics - UK       0.00      0.00      1.00         3\n",
            "              Politics - USA       1.00      0.00      0.00         2\n",
            "       Science - environment       0.00      0.00      1.00         1\n",
            "          Science - medicine       1.00      0.00      0.00         1\n",
            "             Science - space       0.00      1.00      0.00         0\n",
            "         Sports - basketball       1.00      1.00      1.00         2\n",
            "            Sports - cricket       1.00      1.00      1.00         2\n",
            "           Sports - football       1.00      1.00      1.00         1\n",
            "              Sports - rugby       1.00      1.00      1.00         2\n",
            "    Technology - environment       0.00      1.00      0.00         0\n",
            "       Technology - medicine       1.00      0.00      0.00         1\n",
            "          Technology - space       1.00      0.00      0.00         1\n",
            "     Technology - technology       1.00      0.00      0.00         2\n",
            "          Travel - Australia       1.00      0.00      0.00         1\n",
            "            Travel - Germany       1.00      0.00      0.00         1\n",
            "              Travel - Japan       0.00      1.00      0.00         0\n",
            "\n",
            "                    accuracy                           0.56        95\n",
            "                   macro avg       0.63      0.50      0.41        95\n",
            "                weighted avg       0.84      0.56      0.70        95\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"SVM Classification Report:\")\n",
        "print(report_svm)\n",
        "\n",
        "\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(report_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sGY9M2nDAkI",
        "outputId": "b2a66221-684d-4365-db6a-2184aa76193f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "        Awards - environment       1.00      1.00      1.00         2\n",
            "           Awards - medicine       1.00      1.00      1.00         3\n",
            "         Awards - technology       1.00      1.00      1.00         2\n",
            "         Culture - Christmas       1.00      1.00      1.00         8\n",
            "            Culture - Diwali       1.00      1.00      1.00         8\n",
            "               Culture - Eid       1.00      1.00      1.00         4\n",
            "          Culture - Hanukkah       1.00      1.00      1.00         5\n",
            "        Economy - technology       1.00      1.00      1.00         7\n",
            "          Education - Canada       1.00      0.00      0.00         1\n",
            "          Education - France       1.00      0.00      0.00         2\n",
            "         Education - Germany       1.00      0.00      0.00         1\n",
            "           Education - India       1.00      0.00      0.00         1\n",
            "           Education - Japan       1.00      0.00      0.00         1\n",
            "             Education - USA       0.00      1.00      0.00         0\n",
            "        Education - srilanka       1.00      0.00      0.00         1\n",
            "      Entertainment - action       1.00      0.00      0.00         1\n",
            "      Entertainment - comedy       1.00      0.00      0.00         1\n",
            "      Entertainment - horror       0.00      1.00      0.00         0\n",
            "Environment - climate change       1.00      1.00      1.00         3\n",
            "       Geography - Australia       1.00      0.00      0.00         1\n",
            "          Geography - Canada       1.00      0.00      0.00         1\n",
            "              Geography - UK       1.00      0.00      0.00         1\n",
            "             Geography - USA       0.00      1.00      0.00         0\n",
            "             Health - cancer       1.00      1.00      1.00         1\n",
            "       Health - hypertension       1.00      1.00      1.00         1\n",
            "         History - Australia       1.00      0.00      0.00         1\n",
            "            History - France       1.00      0.00      0.00         1\n",
            "             History - India       1.00      0.00      0.00         1\n",
            "                History - UK       1.00      0.00      0.00         1\n",
            "               History - USA       0.00      1.00      0.00         0\n",
            "          History - srilanka       1.00      0.00      0.00         1\n",
            "          Literature - India       1.00      0.00      0.00         1\n",
            "             Literature - UK       1.00      0.00      0.00         2\n",
            "            Literature - USA       0.00      1.00      0.00         0\n",
            "        Politics - Australia       0.00      1.00      0.00         0\n",
            "           Politics - Canada       1.00      0.00      0.00         3\n",
            "           Politics - France       1.00      0.00      0.00         3\n",
            "          Politics - Germany       1.00      1.00      1.00         1\n",
            "            Politics - India       1.00      0.00      0.00         3\n",
            "            Politics - Japan       1.00      1.00      1.00         1\n",
            "               Politics - UK       1.00      0.00      0.00         3\n",
            "              Politics - USA       1.00      0.50      0.67         2\n",
            "       Science - environment       1.00      0.00      0.00         1\n",
            "          Science - medicine       1.00      0.00      0.00         1\n",
            "        Science - technology       0.00      1.00      0.00         0\n",
            "         Sports - basketball       1.00      1.00      1.00         2\n",
            "            Sports - cricket       1.00      1.00      1.00         2\n",
            "           Sports - football       1.00      1.00      1.00         1\n",
            "              Sports - rugby       1.00      1.00      1.00         2\n",
            "    Technology - environment       0.00      1.00      0.00         0\n",
            "       Technology - medicine       1.00      0.00      0.00         1\n",
            "          Technology - space       1.00      0.00      0.00         1\n",
            "     Technology - technology       1.00      0.00      0.00         2\n",
            "          Travel - Australia       1.00      0.00      0.00         1\n",
            "            Travel - Germany       1.00      0.00      0.00         1\n",
            "                Travel - USA       0.00      1.00      0.00         0\n",
            "\n",
            "                    accuracy                           0.57        95\n",
            "                   macro avg       0.84      0.47      0.32        95\n",
            "                weighted avg       1.00      0.57      0.57        95\n",
            "\n",
            "Random Forest Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "        Awards - environment       1.00      1.00      1.00         2\n",
            "           Awards - medicine       1.00      1.00      1.00         3\n",
            "         Awards - technology       1.00      1.00      1.00         2\n",
            "         Culture - Christmas       1.00      1.00      1.00         8\n",
            "            Culture - Diwali       1.00      1.00      1.00         8\n",
            "               Culture - Eid       1.00      1.00      1.00         4\n",
            "          Culture - Hanukkah       1.00      1.00      1.00         5\n",
            "        Economy - technology       1.00      1.00      1.00         7\n",
            "       Education - Australia       0.00      1.00      0.00         0\n",
            "          Education - Canada       0.00      0.00      1.00         1\n",
            "          Education - France       1.00      0.00      0.00         2\n",
            "         Education - Germany       1.00      0.00      0.00         1\n",
            "           Education - India       1.00      0.00      0.00         1\n",
            "           Education - Japan       1.00      0.00      0.00         1\n",
            "              Education - UK       0.00      1.00      0.00         0\n",
            "        Education - srilanka       1.00      1.00      1.00         1\n",
            "      Entertainment - action       1.00      0.00      0.00         1\n",
            "      Entertainment - comedy       1.00      0.00      0.00         1\n",
            "      Entertainment - horror       0.00      1.00      0.00         0\n",
            "Environment - climate change       1.00      1.00      1.00         3\n",
            "       Geography - Australia       1.00      1.00      1.00         1\n",
            "          Geography - Canada       1.00      0.00      0.00         1\n",
            "          Geography - France       0.00      1.00      0.00         0\n",
            "         Geography - Germany       0.00      1.00      0.00         0\n",
            "              Geography - UK       1.00      0.00      0.00         1\n",
            "             Health - cancer       0.00      0.00      1.00         1\n",
            "       Health - hypertension       0.00      0.00      1.00         1\n",
            "         History - Australia       1.00      0.00      0.00         1\n",
            "            History - Canada       0.00      1.00      0.00         0\n",
            "            History - France       1.00      0.00      0.00         1\n",
            "           History - Germany       0.00      1.00      0.00         0\n",
            "             History - India       0.00      0.00      1.00         1\n",
            "                History - UK       1.00      0.00      0.00         1\n",
            "          History - srilanka       1.00      0.00      0.00         1\n",
            "      Literature - Australia       0.00      1.00      0.00         0\n",
            "         Literature - France       0.00      1.00      0.00         0\n",
            "          Literature - India       1.00      0.00      0.00         1\n",
            "             Literature - UK       1.00      0.00      0.00         2\n",
            "        Politics - Australia       0.00      1.00      0.00         0\n",
            "           Politics - Canada       0.00      0.00      1.00         3\n",
            "           Politics - France       0.00      0.00      1.00         3\n",
            "          Politics - Germany       0.50      1.00      0.67         1\n",
            "            Politics - India       1.00      0.00      0.00         3\n",
            "            Politics - Japan       0.50      1.00      0.67         1\n",
            "               Politics - UK       0.00      0.00      1.00         3\n",
            "              Politics - USA       1.00      0.00      0.00         2\n",
            "       Science - environment       0.00      0.00      1.00         1\n",
            "          Science - medicine       1.00      0.00      0.00         1\n",
            "             Science - space       0.00      1.00      0.00         0\n",
            "         Sports - basketball       1.00      1.00      1.00         2\n",
            "            Sports - cricket       1.00      1.00      1.00         2\n",
            "           Sports - football       1.00      1.00      1.00         1\n",
            "              Sports - rugby       1.00      1.00      1.00         2\n",
            "    Technology - environment       0.00      1.00      0.00         0\n",
            "       Technology - medicine       1.00      0.00      0.00         1\n",
            "          Technology - space       1.00      0.00      0.00         1\n",
            "     Technology - technology       1.00      0.00      0.00         2\n",
            "          Travel - Australia       1.00      0.00      0.00         1\n",
            "            Travel - Germany       1.00      0.00      0.00         1\n",
            "              Travel - Japan       0.00      1.00      0.00         0\n",
            "\n",
            "                    accuracy                           0.56        95\n",
            "                   macro avg       0.63      0.50      0.41        95\n",
            "                weighted avg       0.84      0.56      0.70        95\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "\n",
        "grid_svm = GridSearchCV(SVC(), param_grid_svm, refit=True, verbose=2, n_jobs=-1)\n",
        "grid_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "y_pred_svm_grid = grid_svm.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "report_svm_grid = classification_report(y_test, y_pred_svm_grid, zero_division=1)\n",
        "print(\"Tuned SVM Classification Report:\")\n",
        "print(report_svm_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTQLQnkoDLJa",
        "outputId": "a177fa54-61dd-418f-c82c-9b1337b68c83"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned SVM Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "        Awards - environment       1.00      1.00      1.00         2\n",
            "           Awards - medicine       1.00      1.00      1.00         3\n",
            "         Awards - technology       1.00      1.00      1.00         2\n",
            "         Culture - Christmas       1.00      1.00      1.00         8\n",
            "            Culture - Diwali       1.00      1.00      1.00         8\n",
            "               Culture - Eid       1.00      1.00      1.00         4\n",
            "          Culture - Hanukkah       1.00      1.00      1.00         5\n",
            "        Economy - technology       1.00      1.00      1.00         7\n",
            "          Education - Canada       1.00      1.00      1.00         1\n",
            "          Education - France       1.00      1.00      1.00         2\n",
            "         Education - Germany       1.00      1.00      1.00         1\n",
            "           Education - India       1.00      1.00      1.00         1\n",
            "           Education - Japan       1.00      1.00      1.00         1\n",
            "        Education - srilanka       1.00      1.00      1.00         1\n",
            "      Entertainment - action       1.00      1.00      1.00         1\n",
            "      Entertainment - comedy       1.00      1.00      1.00         1\n",
            "Environment - climate change       1.00      1.00      1.00         3\n",
            "       Geography - Australia       1.00      1.00      1.00         1\n",
            "          Geography - Canada       1.00      1.00      1.00         1\n",
            "              Geography - UK       1.00      1.00      1.00         1\n",
            "             Health - cancer       1.00      1.00      1.00         1\n",
            "       Health - hypertension       1.00      1.00      1.00         1\n",
            "         History - Australia       1.00      1.00      1.00         1\n",
            "            History - France       1.00      1.00      1.00         1\n",
            "             History - India       1.00      1.00      1.00         1\n",
            "                History - UK       1.00      1.00      1.00         1\n",
            "          History - srilanka       1.00      1.00      1.00         1\n",
            "          Literature - India       1.00      1.00      1.00         1\n",
            "             Literature - UK       1.00      0.00      0.00         2\n",
            "            Literature - USA       0.00      1.00      0.00         0\n",
            "           Politics - Canada       1.00      1.00      1.00         3\n",
            "           Politics - France       1.00      1.00      1.00         3\n",
            "          Politics - Germany       1.00      1.00      1.00         1\n",
            "            Politics - India       1.00      1.00      1.00         3\n",
            "            Politics - Japan       1.00      1.00      1.00         1\n",
            "               Politics - UK       1.00      1.00      1.00         3\n",
            "              Politics - USA       1.00      1.00      1.00         2\n",
            "       Science - environment       1.00      1.00      1.00         1\n",
            "          Science - medicine       1.00      1.00      1.00         1\n",
            "         Sports - basketball       1.00      1.00      1.00         2\n",
            "            Sports - cricket       1.00      1.00      1.00         2\n",
            "           Sports - football       1.00      1.00      1.00         1\n",
            "              Sports - rugby       1.00      1.00      1.00         2\n",
            "       Technology - medicine       1.00      1.00      1.00         1\n",
            "          Technology - space       0.00      0.00      1.00         1\n",
            "     Technology - technology       0.00      0.00      1.00         2\n",
            "          Travel - Australia       1.00      1.00      1.00         1\n",
            "            Travel - Germany       1.00      1.00      1.00         1\n",
            "\n",
            "                    accuracy                           0.95        95\n",
            "                   macro avg       0.94      0.94      0.96        95\n",
            "                weighted avg       0.97      0.95      0.98        95\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "file_path = '/content/combined_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "data.dropna(subset=['user_query', 'topic'], inplace=True)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['user_query'], data['topic'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "\n",
        "grid_svm = GridSearchCV(SVC(), param_grid_svm, refit=True, verbose=2, n_jobs=-1)\n",
        "grid_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "y_pred_svm_grid = grid_svm.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "report_svm_grid = classification_report(y_test, y_pred_svm_grid, zero_division=1)\n",
        "print(\"Tuned SVM Classification Report:\")\n",
        "print(report_svm_grid)\n",
        "\n",
        "\n",
        "query = [\"prime minister of India\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "predicted_topic = grid_svm.predict(query_tfidf)\n",
        "\n",
        "print(f\"The predicted topic for the query '{query[0]}' is: {predicted_topic[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTJRJKI4E7K_",
        "outputId": "1b635b65-c53d-4c85-b74f-b11e8ca188e1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned SVM Classification Report:\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "        Awards - environment       1.00      1.00      1.00         2\n",
            "           Awards - medicine       1.00      1.00      1.00         3\n",
            "         Awards - technology       1.00      1.00      1.00         2\n",
            "         Culture - Christmas       1.00      1.00      1.00         8\n",
            "            Culture - Diwali       1.00      1.00      1.00         8\n",
            "               Culture - Eid       1.00      1.00      1.00         4\n",
            "          Culture - Hanukkah       1.00      1.00      1.00         5\n",
            "        Economy - technology       1.00      1.00      1.00         7\n",
            "          Education - Canada       1.00      1.00      1.00         1\n",
            "          Education - France       1.00      1.00      1.00         2\n",
            "         Education - Germany       1.00      1.00      1.00         1\n",
            "           Education - India       1.00      1.00      1.00         1\n",
            "           Education - Japan       1.00      1.00      1.00         1\n",
            "        Education - srilanka       1.00      1.00      1.00         1\n",
            "      Entertainment - action       1.00      1.00      1.00         1\n",
            "      Entertainment - comedy       1.00      1.00      1.00         1\n",
            "Environment - climate change       1.00      1.00      1.00         3\n",
            "       Geography - Australia       1.00      1.00      1.00         1\n",
            "          Geography - Canada       1.00      1.00      1.00         1\n",
            "              Geography - UK       1.00      1.00      1.00         1\n",
            "             Health - cancer       1.00      1.00      1.00         1\n",
            "       Health - hypertension       1.00      1.00      1.00         1\n",
            "         History - Australia       1.00      1.00      1.00         1\n",
            "            History - France       1.00      1.00      1.00         1\n",
            "             History - India       1.00      1.00      1.00         1\n",
            "                History - UK       1.00      1.00      1.00         1\n",
            "          History - srilanka       1.00      1.00      1.00         1\n",
            "          Literature - India       1.00      1.00      1.00         1\n",
            "             Literature - UK       1.00      0.00      0.00         2\n",
            "            Literature - USA       0.00      1.00      0.00         0\n",
            "           Politics - Canada       1.00      1.00      1.00         3\n",
            "           Politics - France       1.00      1.00      1.00         3\n",
            "          Politics - Germany       1.00      1.00      1.00         1\n",
            "            Politics - India       1.00      1.00      1.00         3\n",
            "            Politics - Japan       1.00      1.00      1.00         1\n",
            "               Politics - UK       1.00      1.00      1.00         3\n",
            "              Politics - USA       1.00      1.00      1.00         2\n",
            "       Science - environment       1.00      1.00      1.00         1\n",
            "          Science - medicine       1.00      1.00      1.00         1\n",
            "         Sports - basketball       1.00      1.00      1.00         2\n",
            "            Sports - cricket       1.00      1.00      1.00         2\n",
            "           Sports - football       1.00      1.00      1.00         1\n",
            "              Sports - rugby       1.00      1.00      1.00         2\n",
            "       Technology - medicine       1.00      1.00      1.00         1\n",
            "          Technology - space       0.00      0.00      1.00         1\n",
            "     Technology - technology       0.00      0.00      1.00         2\n",
            "          Travel - Australia       1.00      1.00      1.00         1\n",
            "            Travel - Germany       1.00      1.00      1.00         1\n",
            "\n",
            "                    accuracy                           0.95        95\n",
            "                   macro avg       0.94      0.94      0.96        95\n",
            "                weighted avg       0.97      0.95      0.98        95\n",
            "\n",
            "The predicted topic for the query 'prime minister of India' is: Politics - India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = [\"prime minister of India\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "predicted_topic = grid_svm.predict(query_tfidf)\n",
        "\n",
        "print(f\"The predicted topic for the query '{query[0]}' is: {predicted_topic[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3fk5qwcF5es",
        "outputId": "d799bf45-75f6-4b25-f528-d6a1c8521674"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted topic for the query 'prime minister of India' is: Politics - India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_new_tokens=50, temperature=1.0, top_k=50, tokenizer=tokenizer):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "    generated_ids, generated_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, tokenizer=tokenizer)\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "def try_different_parameters(prompt):\n",
        "    parameters_list = [\n",
        "        {'max_new_tokens': 20, 'temperature': 0.7, 'top_k': 50},\n",
        "        {'max_new_tokens': 100, 'temperature': 0.5, 'top_k': 95},\n",
        "        {'max_new_tokens': 150, 'temperature': 1.2, 'top_k': 80},\n",
        "        {'max_new_tokens': 200, 'temperature': 1.0, 'top_k': 100},\n",
        "    ]\n",
        "\n",
        "    for params in parameters_list:\n",
        "        generated_text = generate_text(\n",
        "            prompt,\n",
        "            max_new_tokens=params['max_new_tokens'],\n",
        "            temperature=params['temperature'],\n",
        "            top_k=params['top_k']\n",
        "        )\n",
        "        print(f\"\\nParameters: max_new_tokens={params['max_new_tokens']}, temperature={params['temperature']}, top_k={params['top_k']}\")\n",
        "        print(f\"Bot response: {generated_text}\")\n",
        "\n",
        "\n",
        "prompt = \"how is Diwali celebrated in Australia?\"\n",
        "try_different_parameters(prompt)\n",
        "# Define the prompt/query\n",
        "\n",
        "\n",
        "\n",
        "query_tfidf = vectorizer.transform([prompt])\n",
        "predicted_topic = grid_svm.predict(query_tfidf)\n",
        "\n",
        "print(f\"The predicted topic for the query '{prompt}' is: {predicted_topic[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvvKcEQ0J288",
        "outputId": "c029057c-6cd1-440c-982f-0862a98540d8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters: max_new_tokens=20, temperature=0.7, top_k=50\n",
            "Bot response: how is Diwali celebrated in Australia? with vibrant cultural as Sri traditional, dance gatherings with is, is India during, is, is the\n",
            "\n",
            "Parameters: max_new_tokens=100, temperature=0.5, top_k=95\n",
            "Bot response: how is Diwali celebrated in Australia? with vibrant community events, is with, the with USA is the with with with,ali is Japan and traditional is the such the not,w vibrant celebrated, often is the suchw which the such the is the such Japan, the with with with specific with with specific,ali is the is the widely the is is is the such,w the climate with with with with with highest specific with with with with with with with with with is the such the such is the climate is the such is is\n",
            "\n",
            "Parameters: max_new_tokens=150, temperature=1.2, top_k=80\n",
            "Bot response: how is Diwali celebrated in Australia? with vibrant communityali the foods with can environmental traditional isw private Japan traditional USA is Germany to in foods with is Germany its by,,ali UK Japan to is Japan Sch with with with often theali, foods Eid the climate Germany to driver, often Di where reach k France generally with often the such is USA is, Japan to,, in driver have celebrated traditional USA is, in followed private, destinations Sydney in is of a the with with most courses with in so its clouds, with Eid, popular is is, in rise with with eight is Sri such the in often is Australia destination the to includeë with with, is is reach whichaliw and with by, is,, medicalach vibrant fruit in driver London celebrated to by with\n",
            "\n",
            "Parameters: max_new_tokens=200, temperature=1.0, top_k=100\n",
            "Bot response: how is Diwali celebrated in Australia? with vibrant cultural is France celebrated, increased of in USA is Japan at with with USA is the is served,ali is,w;, Japan to administration, often reachFC with with including celebrated with with is the colorful family private,ali the known change Han the to by is, not destination the such, is stuffing,ali USA is Japan in with with with with inche the climate with with with with with with with with with with with with in with with with cank with with most Japan Sch Canada in with with with with with visiting France service gatherings theak which popularali is India such the known Seven typically is variety private the with with specific with is the with in like, Sydney,\" mainly destination Germany celebrated mainly which is the with with with with breath celebrated with with with with foods, such a the where is Japan was with with which Sri lat, is the with with with specific with with is served with significantly Christmas celebrated with sources with to theak and with by known extensive with provides\n",
            "The predicted topic for the query 'how is Diwali celebrated in Australia?' is: Culture - Diwali\n"
          ]
        }
      ]
    }
  ]
}